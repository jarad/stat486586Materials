---
title: 'Homework #10'
output: html_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!-- ## A useful note -->
<!-- Remove any `View()` or `install.packages()` commands in your `Rmd` file. These commands will prevent the knitting of your file.  -->

## Statistical learning

1. In this question, you are going to program the 5-fold CV from scratch, and then apply it to select tuning parameters for regression splines.
(Hint: You may first want to create a random partition of the data into 5 parts.)

    You will work on the `Wage` dataset in `library(ISLR2)`. Use the `wage` as the response and `age` as the only predictor. 
    Apply the cubic regression spline to model the nonlinear effect of `age`. 
    Include $K$ equally spaced knots strictly within the range of the predictor age (18 to 80). E.g., `seq(18, 80, length.out=4)[-c(1, 4)]` gives $K=2$ equally spaced knots.
    a. Use the 5-fold CV to estimate the test MSE for $K=1, \dots, 15$ interior knots. Visualize the CV error as a function of $K$.
    b. Select the model with the smallest CV error, and fit the model using the full dataset. Visualize the nonlinear regression effect.


1. You are going to generate some data and use them to study the MSE of ridge regression and the lasso.

    The data generating model is:
    
    $$\text{Case 1: } Y = 1 + 1 \cdot X_1 + (-1) \cdot X_2 + 0 \cdot X_3 + \dots + 0 \cdot X_{20} + \epsilon,$$
    where the predictors $X_1, \dots, X_{20}$ and the noise $\epsilon$ all follow independent standard normal distributions.
    Note that only the first two slope parameters are nonzero in the data generating model. 
    
    In each simulation repeat, do:
      - Generate a set of $n=100$ for the training set, and $m=500$ for the test set
      - You are going fit both ridge regression and the lasso to the training data including *all* covariates
      - Apply regularization parameter $\lambda \in \{0.1, 1, 10, 100, 1000\}$
      - Evaluate the test MSE of the model using the test set for both models and all $\lambda$
    
    Repeat the previous process for $B=200$ simulation repeats, and average the test MSE as the final estimate of the test MSE. 
    Visualize the MSE of ridge regression and the lasso over $\log(\lambda)$.
    Compare the lasso and ridge regression. Which method performs better, and how does that relate to the underlying data generation?

1. In this exercise, we will predict the number of applications received (`Apps`)
using the other variables in the `College` data set in `library(ISLR2)`.

    a. Fit a ridge regression model on the training set, with λ chosen
  by cross-validation. Report the CV error obtained.
    a. Fit a lasso model on the training set, with λ chosen by cross-validation.
  Report the CV error obtained and the non-zero coefficient estimates.
    a. Use both the ridge and the lasso models, predict the number of applications received by a hypothetical university. 
    The hypothetical university is public, and the rest of the predictor values are set to the corresponding average values in the dataset.
  
1. The `Weekly` data in `library(ISLR2)` contains weekly percentage returns for the S&P 500 stock index between 1990 and 2010. We will predict the `Direction` (Down or Up) of the stock market on a given week.

    a. Use the full data set to perform a logistic regression with `Direction` as the response and the five lag variables plus `Volume` as predictors. Use the `summary` function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?
    a. Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.
    a. Now fit the logistic regression model using a training data period from 1990 to 2008, with `Lag2` as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).
    a. Repeat the last part using KNN with $K$ chosen using cross-validation.
