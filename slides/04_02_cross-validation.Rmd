---
title: "Cross-validation"
subtitle: "STAT486/586"
author: "Xiongtao Dai"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: [myslides.css, myslides-fonts.css]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      highlightLines: true
      countIncrementalSlides: false
---

```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
knitr::opts_chunk$set(fig.height=4, fig.width=4, fig.align='center',message = FALSE)
ggplot2::theme_set(ggplot2::theme_light(base_size = 16))
```

## Outline

- Measuring the quality of fit 

    - Mean squared error
    
    - bias-variance trade-off

- Cross-validation

    - Leave-out-out CV
    
    - $k$-fold CV

- Case study

---
class: big, middle

## Measuring performance

---

## Measuring the quality of fit

In the most general case, the relationship between $Y$ and $X=(X_1,\dots,X_p)$ follows
$$Y = f(X) + \epsilon.$$

The underlying regression function $f$ is unknown, but we can obtain an estimator $\hat{f}(x)$ (e.g., the least squares estimate for multiple regression). 
We can predict $Y$ using 
$$\hat{Y} = \hat{f}(X).$$

Define the mean squared error (MSE) as
$$E(Y - \hat{Y})^2,$$
which is the expected (long term average) squared error

---

The accuracy of $\hat{Y}$ for prediction $Y$ depends on two quantities:

- The *reducible* error, which can in principle be reduced to 0 given an appropriate model and a large sample

- The *irreducible* error cannot be reduced in any ways. This is due to the intrinsic variability in the noise $\epsilon$ which cannot be predicted. It could be sourced from unmeasured variables

- It can be shown that
$$
\begin{aligned}
E(Y-\hat{Y})^2 & = E[f(X) + \epsilon - \hat{f}(X)]^2 \\
& = \underbrace{E[f(X) - \hat{f}(X)]^2}_{\text{reducible}} + \underbrace{\mathrm{var}(\epsilon)}_{\text{irreducible}}
\end{aligned}
$$

- Modelers' goal is to minimize the reducible error

- However, the MSE is again, unknown, and thus needs to be estimated

---

## Which model should I use?

If we knew MSE, we could use it to compare models. 

- E.g., choose the degree in a polynomial regression, or the number of knots in a spline regression

- It is possible to know MSE only if we simulated the data.

- Data below is not quite linear
```{r, echo=FALSE,fig.height=5, fig.width=5}
f <- function(x) sin(2 * pi * x)
n <- 100
sigma <- 1
X <- sort(runif(n))
Y <- f(X) + rnorm(n, sd=sigma)
plot(X, Y, pch=16)
lines(X, f(X), lwd=2)
```

---
## MSE in a simulation

Fit polynomials with degree up to 9, and obtain the MSE, reducible errors, and irreducible errors
```{r,echo=FALSE, cache=TRUE}
library(tidyverse)
MC <- 100
nNew <- 2000
qVec <- 1:12
set.seed(1)
fitPoly <- function(y, x, q) {
  form <- sprintf("y ~ poly(x, %d)", q)
  form <- as.formula(form)
  lm(form)
}
nInf <- 5000
mInf <- map(qVec, function(q) {
  X <- sort(runif(nInf))
  Y <- f(X) + rnorm(nInf, sd=sigma)
  fitPoly(Y, X, q)
})
res <- map_dfr(seq_len(MC), function(i) {
  # Generate data
  X <- sort(runif(n))
  Y <- f(X) + rnorm(n, sd=sigma)
  imap_dfr(qVec, function(q, i) {
    # Fit model
    m <- fitPoly(Y, X, q)
    YHat <- predict(m)
    # Generate new data to evaluate fit
    XNew <- runif(nNew)
    epsNew <- rnorm(nNew, sd=sigma)
    YNew <- f(XNew) + epsNew
    YHatNew <- predict(m, newdata=data.frame(x=XNew))
    YHatInf <- predict(mInf[[i]], newdata=data.frame(x=XNew))
    c(q=q,
      Reducible=mean((YHatNew - f(XNew))^2), 
      Irreducible=mean(epsNew^2),
      MSE=mean((YHatNew - YNew)^2), 
      Training = mean((YHat - Y)^2),
      SqBias = mean((YHatInf - f(XNew))^2), 
      Variance = mean((YHat - YHatInf)^2)
      )
  })
})
```
```{r,echo=FALSE, fig.height=5, fig.width=6}
plotDat <- res %>%
  pivot_longer(-q) %>%
  group_by(q, name) %>%
  summarize(value = mean(value)) %>%
  ungroup() %>%
  mutate(name = factor(name, 
                       c("MSE", 
                         "Reducible", 
                         "Irreducible", 
                         "Training",
                         "SqBias",
                         "Variance"))) %>%
  mutate(value = ifelse(name == "Irreducible", 
                        mean(value[name == "Irreducible"]),
                        value))

plotDat %>%
  filter(name %in% c("MSE", "Reducible", "Irreducible")) %>%
  ggplot(aes(x = q, y = value, color=name)) + geom_line()
# total <- map(res, map_dbl, "total")
# total <- do.call(rbind, total)
# plot(qVec, colMeans(total), type="l", xlab="degree - 1", ylab="MSE", main="MSE from simulation")
```

- Thus using a polynomial regression up to a cubic term $q=3$ is the best choise
- There is a considerable window of $q$ that results in similar results

---

## Training MSE

- Of course, the true MSE is unaccessible when we are not doing simulation

- The data $(X_1,Y_1),\dots,(X_n,Y_n)$ used to fit the model is called the *training data*

- A possible alternative is the *training MSE*
$$\frac{1}{n}\sum_{i=1}^n(Y_i - \hat{f}(X_i))^2,$$
evaluated at the training data

- But this won't work for evaluating the model, as we will see

- In general, we do not really care how the model perform on the training data

- We are really interested in the performance when we obtain new and unseen *test data*

---

## Test MSE

Let $(X_0, Y_0)$ be a previously unseen *test observation* not used to train
the model.

- The test MSE is 
$$\mathrm{Ave}(Y_0 - \hat{f}(X_0))^2,$$
where the average is taken over many new test observations $(X_0, Y_0)$

- Our goal is to select the model that minimizes the test MSE

- Sometimes, there is a test dataset available. I.e., there is a separate dataset which you did not use to fit the data. You can then evaluate the test MSE using the test dataset

    - This is not always possible, and 
    - not using the test set to fit the model is a waste of data
    
---

## Minimizing training MSE fails

- Minimizing training MSE might seem appealing since it looks like the test MSE

- However, there is a fundamental problem: there is no guarantee that the method with the lowest training MSE will also have the lowest test MSE
    
- The reason is that the training MSE is always smaller for a more flexible model

- but a more flexible model does not necessarily mean a lower test MSE

    - A model interpolating all the data points is going to give a 0 training error. But this will perform poorly given new data
    
---

```{r, echo=FALSE, fig.width=7, fig.height=6}
plotDat %>%
  filter(name %in% c("MSE", "Training")) %>%
  mutate(name = factor(name, c("Training", "MSE"), c("Training MSE", "Test MSE"))) %>%
  ggplot(aes(x = q, y = value, color=name)) + geom_line() + xlab("Flexibility (q)")
```

---

## The bias-variance trade-off

The U-shape observed in the test MSE is the result of two competing properties of a model: *bias* and *variance*

The *expected test MSE* is the sum of three terms:
$$
\begin{aligned}
E[Y_0-\hat{f}(X_0)]^2 = \underbrace{\{E[\hat{f}(X_0) - f(X_0)]\}^2}_{\text{squared bias}} + \underbrace{\text{var}(\hat{f}(X_0))}_{\text{variance}} + \underbrace{\mathrm{var}(\epsilon)}_{\text{irreducible error}}
\end{aligned}
$$

- The bias and variance terms sum up to the reducible error

- When selecting a model, our goal is to simultaneously achieve low bias and low variance

---
$$
\begin{aligned}
E[Y_0-\hat{f}(X_0)]^2 = \underbrace{\{E[\hat{f}(X_0) - f(X_0)]\}^2}_{\text{squared bias}} + \underbrace{\text{var}(\hat{f}(X_0))}_{\text{variance}} + \underbrace{\mathrm{var}(\epsilon)}_{\text{irreducible error}}.
\end{aligned}
$$

- *Bias* is the lack of flexibility in our model to approximate the true $f$, regardless of how many training observations we have. 
    - E.g., using a line to approximate a curvilinear relationship will incur a bias. More data does not help the fact that we cannot fit a curve with a line
    - flexibility $\uparrow$, bias $\downarrow$
    
- *Variance* refers to how much $\hat{f}$ varies if we use a different training data set. To fit a more flexible model needs to estimate more parameters
    - flexibility $\uparrow$, variance $\uparrow$

- The relationship between bias and variance as the flexibility of the model change is referred to as the *bias-variance trade-off*

- We want to select a flexible but not too flexible model in a happy middle ground

---

Our simulation:
```{r, echo=FALSE, fig.width=7, fig.height=6}
plotDat %>%
  filter(name %in% c("MSE", "SqBias", "Variance")) %>%
  mutate(name = factor(name, c("MSE", "SqBias", "Variance"))) %>%
  ggplot(aes(x = q, y = value, color=name)) + geom_line() + xlab("Flexibility (q)")
```

- In real life, it is generally not possible to compute the test MSE, bias, or variance 

- Nevertheless, one should always keep the bias-variance trade-off in mind

---
class: big,middle

## Cross-validation

---

## The validation set approach

- Our goal is to estimate the test MSE

- The *validation set approach* is to randomly dividing the set of all observations into two parts, a *training set* and a *validation set* (or *hold-out set*)

- The model is fit on the training set, and the evaluation of model is done on the validation set

.center[
![:scale 70%](images/validationSet.png)
]

- Two potential drawbacks:
    - The estimate of the test error rate can be highly variable, depending on the split
    - Tend to underestimate the test error, since methods tend to perform
worse given fewer observations to train

---

## Leave-One-Out Cross-Validation

*Leave-one-out cross-validation* (LOOCV) addresses the drawbacks of the validation set approach.

- LOOCV 
    1. split the set of observations into two parts, a validation set having a single observation $(X_1,Y_1)$, and a training set containing the rest $\{(X_2,Y_2),\dots,(X_n,Y_n)\}$
    1. Fit on the training set, and make prediction for the validation data. Evaluate test error on the validation data
    1. Repeat the procedure by holding out every original observation, one at a time, as the training set
    1. Average test errors over all observations
.center[
![:scale 70%](images/LOOCV.png)
]

---

## Why LOOCV works

- In the $i$th split, for $i=1,\dots,n$, the model is trained using $\{(X_1,Y_1),\dots,(X_{i-1},Y_{i-1}),(X_{i+1},Y_{i+1}), (X_n,Y_n)\}$

- In the $i$th fitting process, the prediction is made for $(X_i, Y_i)$ independent of the data used for training. So $\text{MSE}_i = (Y_i - \hat{Y}_i)^2$ is an approximately unbiased estimate for the test MSE. 
    - The $i$th test error alone could have a large variability

- The LOOCV estimate for the test MSE pools all $n$ test errors, and is
$$\text{CV}_{(n)} = \frac{1}{n}\sum_{i=1}^n \text{MSE}_i.$$
By averaging, this test error has a smaller variability

- LOOCV can be used to evaluate any regression or classification methods

---

## LOOCV benefits and drawbacks

Benefits of LOOCV over the validation set approach:

- Less bias, since $n-1$ observations are used for training each time

- Less randomness, since the splits are non-random
    
Potential drawbacks of LOOCV:

- Expensive computation, since the model needs to be fit $n$ times

- Just so you know, there is a magical formula to perform LOOCV in a single model fit. It applies to a wide range of linear models:
$$\text{CV}_{(n)} = \frac{1}{n}\sum_{i=1}^n \left(\frac{Y_i-\hat{Y}_i}{1-h_i}\right)$$

- But still, in general, LOOCV requires $n$ model fits

---

## $k$-fold cross-validation

An alternative to LOOCV is *k-fold cross-validation* (CV). This approach involves 

1. Randomly divide the set of observations into $k$ groups, or *folds*, of approximately equal size
1. The first fold is treated as a validation set, and the model is fit on the remaining $k-1$ folds
1. Evaluate the prediction on the validation set (held-out fold)
1. Repeat the previous process for each of the $k$ folds
1. Obtain $k$-fold CV estimate
$$\text{CV}_{(k)} = \frac{1}{k}\sum_{i=1}^n \text{MSE}_i.$$

- LOOCV is the same as the $n$-fold CV

- Typically, $k=5$ or $k=10$ fold CV are used

---

## Advantages of $k$-fold CV

Advantages of $k$-fold CV:

- $k$-fold CV is computationally more efficient than LOOCV, since the former fits the model only $k$ times

- More importantly, $k$-fold CV is more accurate for estimating the true test MSE curve than the LOOCV

The reason lies in bias-variance tradeoff. 
- When performing LOOCV, we are averaging the prediction made by $n$ fitted models
  - Each model has a *very small bias* since they use all but 1 observations
  - Each model differs by only $1$ observation, so they are highly correlated with each other
  - The average test error of highly correlated variables will have a *high variability*
- In contrast, the $k$ models fitted in $k$-fold CV are less correlated, so averaging is going to help more in reducing the variance, though the bias is slightly increased. Again, $k=5$ or $10$ are good choices in practice

---

## A general way to perform a learning task

In fact, a learning task often involve the many models/options to choose from. The procedure often go as follows

1. Define an objective to optimize (e.g., minimize MSE for regression, maximize likelihood for estimating parameters)
1. Define a fair way to evaluate each model (e.g., for supervised learning, LOOCV or $k$-fold CV)
1. Choose the model that perform the best according to your evaluation
1. Fit the best model using all data

---

## Case study: Evaluating polynomial regression

Consider the wage data. Our goal is to predict the wage with age using a polynomial regression

$$\text{wage}_i = \beta_0 + \beta_1\text{age}_i + ... + \beta_q\text{age}_i^q + \epsilon_i.$$ 

For illustration, use just the first 200 observations for speed.

```{r, wageLOOCV, eval=FALSE}
qVec <- 1:10
dat <- Wage[1:200, ]
n <- nrow(dat)

# LOOCV
MSEi <- map_dfr(seq_len(n), function(i) {
  datTrain <- dat[-i, ]
  datTest <- dat[i, ]
  
  res <- map_dbl(qVec, function(q) {
    m <- lm(wage ~ poly(age, q), datTrain)
    err <- datTest$wage - predict(m, datTest) 
    mean(err^2)
  })
  setNames(res, qVec)
})
MSECV <- colMeans(MSEi)
plot(qVec, MSECV, type="b", main="MSE estimated by LOOCV")
```

---

Test error for $n=200$ observations
```{r, wageLOOCV, echo=FALSE, fig.height=6, fig.width=6, eval=TRUE, cache=TRUE}
```

- Using the bias-variance trade-off principle, what $q$ do you think will be selected when we include all $n=3000$ observations?

- TODO: cover `cv.glm`

---
class: inverse

## Your turn

TODO

<img src="images/green.png" width=20> Generate a random sample of size $n=10$ from the following distributions: (a). Beta(1,2) and (b). Uniform(0,100)

<img src="images/green.png" width=20> Sample 5 random rows from `iris` without replacement

<img src="images/green.png" width=20> Randomly shuffle the 5 rows

<img src="images/green.png" width=20> Sample 10 random rows from `iris` with replacement

