---
title: "Statistical Inference </br> with Random Numbers"
subtitle: "STAT486/586"
author: "Xiongtao Dai"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: [myslides.css, myslides-fonts.css]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      highlightLines: true
      countIncrementalSlides: false
---

```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
knitr::opts_chunk$set(fig.height=4, fig.width=4, fig.align='center')
```

## Outline

- Hypothesis testing

- Sample size calculation

- Confidence interval

---

## Hypothesis testing: A review

Hypothesis testing is one of the most critical skill of a statistician.

When performing **hypothesis testing**, we are testing the **null hypothesis** $(H_0)$, the default position, against the **alternative hypothesis** $(H_A)$, the statement we want to prove. 
We then see whether our data provides evidence supporting $H_A$. 

There are four steps to hypothesis testing. 
As a running example, we want to test whether the *underlying mean income $\mu$ of all graduates* equals 50 thousand. The data we collect are the income $X_1, \dots, X_{30}$ of $n=30$ recent graduates.

1. State the null and the alternative hypothesis. E.g.,
$$H_0: \mu = 50 \quad \text{vs} \quad H_A: \mu \ne 50.$$
<!-- $$H_0: \mu_1 \le \mu_2 \quad \text{vs} \quad H_A: \mu_1 > \mu_2$$ -->
<!-- $$H_0: \mu_1 \ge \mu_2 \quad \text{vs} \quad H_A: \mu_1 < \mu_2.$$ -->

---

2\. Calculate the **test statistic**, which summarizes the evidence from the data against $H_0$. The test statistic we use in this case is 
$$T = \frac{\bar{X} - 50}{s / \sqrt{n}},$$
where $s^2 = (n-1)^{-1}\sum_{i=1}^n (X_i - \bar{X})^2$ is the sample variance. You will plug in the data values and obtain the observed test statistic $t_s$.

  - Under $H_0$, *if&nbsp; $\bar{X}$ is normally distributed*, then $T$ follows a $t$-distribution with degree of freedom $df=n-1$ and will be close to zero. 
  .center[
![:scale 40%](https://upload.wikimedia.org/wikipedia/commons/4/41/Student_t_pdf.svg) (from wikipedia)
]
  - Under $H_A$, $t_s$ will have a large absolute value.

---

3\. Calculate the **p-value**, which is the probability of observing a test statistic at least as extreme as $t_s$ if $H_0$ were true. 

  Since our alternative is two-sided, 
    $$pv=Pr(|T| > |t_s|) = 2 Pr(T > |t_s|).$$

</br>
4\. Draw conclusions. Small p-value is unlikely under $H_0$, and this indicates that $H_A$ is true. We choose the (nominal) **significant level** $\alpha$, say 0.05.

- If $p$-value $\le \alpha$, **reject** $H_0$ and conclude $H_A$. "significant"
- If $p$-value $> \alpha$, **fail to reject** $H_0$ and there is insufficient evidence for $H_A$. No conclusion can be made! "insignificant"

---
## Type I and Type II errors

- *Type I error*: reject $H_0$ when $H_0$ is true
- *Type II error*: do not reject $H_0$ when $H_0$ is false


  .plainTable[
  | |  | Decision:|  |
  |:----:|:----:|:----:|:----:|
  | |  | Do not reject $H_0$ | Reject $H_0$|
  |Reality: | $H_0$ true |  OK |*Type I error*|
  |       | $H_0$ false|   *Type II error*   |OK   |
  ]


---

  .center[
![:scale 90%](https://i.stack.imgur.com/Kq0OH.jpg) 
</br>([stackexchange](https://economics.stackexchange.com/questions/27677/type-i-error-type-ii-error-pregnancy-test-analogy-is-it-legit))
]

---
## Size and power

- The *size* of the test is
$$Pr(\text{Type I error}) = Pr(\text{reject } H_0|H_0\text{ is true}).$$
When you set $\alpha = 0.05$, the goal is that $Pr(\text{Type I error})\le0.05$.

- The *power* of the test is $1 - Pr(\text{Type II error})$
$$Pr(\text{Type II error}) = Pr(\text{not reject } H_0|H_A\text{ is true}).$$
A common goal when planning for the sample size in an experiment is that the power is greater than $\beta = 0.8$ (say).


---

## Case study: Hypothesis testing

Suppose that the income $X_1,\dots,X_{30}$ follows i.i.d. $\text{Gamma}(\alpha=10, \beta=1) + 40$. The true mean is 50. Let's see how often does the $t$-test reject and find out the size of the test (type I error rate) at the $\alpha=0.1$ significance level.

```{r}
set.seed(1)
alpha <- 0.1
n <- 30
mu0 <- 50
income <- rgamma(n, 10, 1) + 40
ts <- (mean(income) - mu0) / (sd(income) / sqrt(n))
(pv <- pt(abs(ts), df=n - 1, lower.tail = FALSE) * 2)
```

The test does not reject in one experiment since $pv > \alpha=0.1$.
But to find out the size, we need to replicate the experiment.

---

Wrap the p-value calculation in a function and repeat 500 times
```{r}
library(purrr)

set.seed(1)
MC <- 500
pvs <- map_dbl(seq_len(MC), function(i) {
  income <- rgamma(n, 10, 1) + 40
  ts <- (mean(income) - mu0) / (sd(income) / sqrt(n))
  pv <- pt(abs(ts), df=n - 1, lower.tail = FALSE) * 2
})
mean(pvs <= alpha)
```

So the type I error is slightly inflated, but probably not of a great concern.

Since the computation power nowadays is rich, run at least 500 repeats if affordable. 

---

Now, what about the power? Suppose now that $X_i \sim \text{Gamma}(\alpha=10, \beta=1) + 41$. Now the true mean is 51.
```{r}
set.seed(1)
pvs <- map_dbl(seq_len(MC), function(i) {
  income <- rgamma(n, 10, 1) + 41
  ts <- (mean(income) - mu0) / (sd(income) / sqrt(n))
  pv <- pt(abs(ts), df=n - 1, lower.tail = FALSE) * 2
})
(power <- mean(pvs <= alpha))
```

We only reject `r power*100`% of the time. What if we want to recruits so a larger number of respondents so that the power is at least $\beta=.8$, under the setup we consider here?

---

## Sample size calculation

The goal of sample size calculation is to find the sample size needed in order for the test to achieve a given power.
The power is calculated under a hypothesized scenario $H_A$ which usually reflects the smallest effect size, i.e., difference from $H_0$, that is of practical interest (in this e.g., $H_A: \mu=51$).

```{r}
set.seed(1)
calcPower <- function(n, MC) {
  pvs <- map_dbl(seq_len(MC), function(i) {
    income <- rgamma(n, 10, 1) + 41
    ts <- (mean(income) - mu0) / (sd(income) / sqrt(n))
    pv <- pt(abs(ts), df=n - 1, lower.tail = FALSE) * 2
  })
  mean(pvs <= alpha)
}
calcPower(n, MC) # same answer as before, since we set seed
```

---

We are going to repeat the power calculation for a number of sample sizes, and then find the smallest sample size that give us enough power.

```{r}
beta <- 0.8
nVec <- seq(30, 100, by=5)
powerVec <- map_dbl(nVec, calcPower, MC=MC)
nVec[which(powerVec >= beta)[1]]
```
---

## Confidence intervals

Suppose you have a sample $\{X_1,\dots,X_n\}$ and you have a parameter of interest $\theta$ (say, the mean). A 95% *confidence interval* is an interval estimate $(L,U)$ constructed from the sample so that the *coverage* of the interval
$$P(\theta \in (L,U)) \ge  0.95$$

- If $\theta = \mu$ in the one-sample setting or $\mu_1 - \mu_2$ in the two-sample setting, then often a c.i. is constructed using the $t$-statistic.

- If you have a hypothesis test for $H_0:\theta = \theta_0$ at the $\alpha$-significance level, then
$$\{\theta_0\vert \text{the test does not reject }H_0:\theta = \theta_0\}$$
is a $(1-\alpha)100\%$ c.i. 
So you can invert a hypothesis test to obtain a confidence interval. 

- Vise versa, if you have a $(1-\alpha)\%$ c.i. $(L,U)$, then a hypothesis test at the  $\alpha$ level is obtained as follows: rejects $H_0$ if $\theta_0 \notin (L,U)$.
